\section{Introduction}\label{sec:introduction}

Machine learning aims to learn the correct function from data by minimizing the loss between the predicted output and the actual data.
In the process of finding function parameters that will minimize the loss, optimization techniques such as Gradient Descent are utilized.
On the other hand, it is also possible to leverage machine learning techniques in solving optimization problems.
Researchers have conducted numerous studies in this direction~\cite{bengio_machine_2020}.
Particularly, the field of Combinatorial Optimization, which includes NP-hard problems such as the Traveling Salesman Problem and the Knapsack Problem, is one of the playgrounds where machine learning techniques can provide the most significant contributions.

Combinatorial Optimization problems such as the Traveling Salesman Problem, Knapsack Problem, and Set Covering Problem are concerned with finding the optimal solution within the solution set that contains combinations of decision variables’ values.
It is possible to encounter Combinatorial Optimization problems in various fields such as production, logistics, supply chain, and finance.
As the number of decision variables in Combinatorial Optimization problems increases, the number of combinations and the solution time of the optimization problem increase very rapidly.
Therefore, many Combinatorial Optimization problems are NP-hard.
However, the operations research community has developed efficient algorithms that can be used to solve Combinatorial Optimization problems.
The most famous of these algorithms, and the one preferred by commercial optimization software such as CPLEX, is Branch \& Bound~\cite{wolsey_integer_1999}.

Branch \& Bound is an algorithm developed to solve problems that can be formulated as Mixed Integer Programming (MIP) problems, such as Combinatorial Optimization problems.
Branch \& Bound essentially focuses on iteratively dividing the problem into smaller problems (branching) and solving these smaller problems to determine upper and lower bounds on the optimal value of the main problem (bounding).
When upper and lower bounds are equal or when there is no non-investigated node, the optimal value of the problem is reached, and the best feasible solution is the optimal solution.
The choice of the variable to be used for branching in the Branch \& Bound algorithm is one of the most important factors affecting the length of the search tree generated by the algorithm and its performance.
For example, the preference of naive strategies such as most infeasible branching (this branching strategy chooses the variable with the fractional part closest to 0.5) during the selection of the branching variable can reduce the performance of the algorithm by up to 8 times compared to modern branching strategies~\cite{achterberg_mixed_2013}.
Therefore, many researchers have focused on developing strategies for selecting better branching variables in the Branch \& Bound algorithm.
Some of these strategies include Pseudo-cost branching~\cite{benichou_experiments_1971}, Reliability Branching~\cite{achterbergBranchingRulesRevisited2005}, Backdoor Branching~\cite{fischettiBackdoorBranching2011}, Information-based Branching~\cite{kilinckarzanInformationbasedBranchingSchemes2009}, and Strong Branching~\cite{applegateFindingCutsTSP1995}.
Among these strategies, the one that keeps the search tree generated by the algorithm the shortest is Strong Branching~\cite{applegateFindingCutsTSP1995}.
Strong Branching selects the variable that will provide the greatest improvement in the dual bound as the branching variable by trying each potential branching variable one by one at each branching moment.
This strategy keeps the length of the search tree short, but this method is time-consuming as it tries each candidate variable one by one.
Due to its time cost, Strong Branching is impractical in practice.
A study has shown that although the Strong Branching strategy produces on average 65\% fewer search tree nodes compared to the Hybrid Branching strategy preferred by the CPLEX software, it takes 44\% longer on average~\cite{DBLP:phd/de/Achterberg2007}.

To eliminate the time cost disadvantage of the Strong Branching strategy, researchers have tried machine learning-based methods.
In these studies, researchers aimed to learn functions that would mimic the Strong Branching strategy and make faster branching decisions.
Alvarez et al.~\cite{alvarezMachineLearningBasedApproximation2017} approached the problem as a regression problem and tried to learn a function that would mimic Strong Branching using the ExtraTrees algorithm.
Khalil et al.~\cite{khalilLearningBranchMixed2016} dealt with the problem as a ranking problem and tried to mimic Strong Branching using the svmRank algorithm.
Gasse et al.~\cite{gasseExactCombinatorialOptimization2019} considered the problem as a classification problem and tried to mimic Strong Branching using bipartite graph representation of the problem with Graph Convolutional Neural Network (GCNN)~\cite{kipfSemiSupervisedClassificationGraph2017} algorithm.
In particular, the results of the study by Gasse et al.~\cite{gasseExactCombinatorialOptimization2019} were noteworthy.
Gasse et al.~\cite{gasseExactCombinatorialOptimization2019} not only demonstrated that their proposed method outperformed other machine learning-based approaches across four types of NP-hard problems, but also showed that the method achieved competitive results compared to the default branching strategy of the SCIP optimization software, Reliability Pseudocost Branching (RPB).

In this study, it is aimed to achieve better results by implementing modern Graph Neural Network (GNN) architectures, continuing from where Gasse et al.~\cite{gasseExactCombinatorialOptimization2019} left off.
In the GCNN architecture, all neighboring nodes have equal importance for a node.
However, in practice, this situation is not valid for most problems.
In many problems, the neighboring nodes of a node have different importance for the node itself.
For example, in a web network, not all web pages referring to a web page have the same importance.
References from web pages with high traffic and citation count are more important.
Unlike GCNN, the Graph Attention Network (GAT)~\cite{velickovicGraphAttentionNetworks2018} architecture allows neighboring nodes to have different importance levels.
Therefore, it is thought that better results can be obtained in solving Combinatorial Optimization problems with the GAT algorithm.
This study aims to better mimic the Strong Branching strategy with GAT and to take the success achieved by Gasse et al.~\cite{gasseExactCombinatorialOptimization2019} with GCNN to a higher level.

The experiments conducted in this study have demonstrated that, as hypothesized, Graph Attention Network-based models more effectively imitate the Strong Branching strategy compared to the GCNN.
Three distinct combinatorial optimization problems were solved using the Graph Attention Network with fewer nodes compared to the Graph Convolutional Neural Network.
When comparing the MHGAT8 model with SCIP’s default brancher, RPB, it can be observed that the model is competitive with RPB in the “Medium” problem category.
When the problem complexity increases, the performance of the MHGAT8 model approaches that of the RPB algorithm.