%! This file should contain the bibliography entries for the manuscript.

@misc{bengio_machine_2020,
	title = {Machine {Learning} for {Combinatorial} {Optimization}: a {Methodological} {Tour} d'{Horizon}},
	shorttitle = {Machine {Learning} for {Combinatorial} {Optimization}},
	abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
	urldate = {2023-12-10},
	publisher = {arXiv},
	author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
	month = mar,
	year = {2020},
	note = {arXiv:1811.06128 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, notion},
	file = {arXiv.org Snapshot:/Users/gokhanmurali/Zotero/storage/A4EW8C4B/1811.html:text/html;Bengio et al_2020_Machine Learning for Combinatorial Optimization.pdf:/Users/gokhanmurali/Zotero/storage/D7ZWLNB7/Bengio et al_2020_Machine Learning for Combinatorial Optimization.pdf:application/pdf},
}

@book{wolsey_integer_1999,
	title = {Integer and {Combinatorial} {Optimization}},
	isbn = {978-0-471-35943-2},
	abstract = {Rave reviews for INTEGER AND COMBINATORIAL OPTIMIZATION  "This book provides an excellent introduction and survey of traditional fields of combinatorial optimization . . . It is indeed one of the best and most complete texts on combinatorial optimization . . . available. [And] with more than 700 entries, [it] has quite an exhaustive reference list."-Optima  "A unifying approach to optimization problems is to formulate them like linear programming problems, while restricting some or all of the variables to the integers. This book is an encyclopedic resource for such formulations, as well as for understanding the structure of and solving the resulting integer programming problems."-Computing Reviews  "[This book] can serve as a basis for various graduate courses on discrete optimization as well as a reference book for researchers and practitioners."-Mathematical Reviews  "This comprehensive and wide-ranging book will undoubtedly become a standard reference book for all those in the field of combinatorial optimization."-Bulletin of the London Mathematical Society  "This text should be required reading for anybody who intends to do research in this area or even just to keep abreast of developments."-Times Higher Education Supplement, London  Also of interest . . .  INTEGER PROGRAMMING Laurence A. Wolsey Comprehensive and self-contained, this intermediate-level guide to integer programming provides readers with clear, up-to-date explanations on why some problems are difficult to solve, how techniques can be reformulated to give better results, and how mixed integer programming systems can be used more effectively. 1998 (0-471-28366-5) 260 pp.},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Wolsey, Laurence A. and Nemhauser, George L.},
	month = jul,
	year = {1999},
	keywords = {Mathematics / Discrete Mathematics, Mathematics / General, notion},
}

@incollection{achterberg_mixed_2013,
	address = {Berlin, Heidelberg},
	title = {Mixed {Integer} {Programming}: {Analyzing} 12 {Years} of {Progress}},
	isbn = {978-3-642-38189-8},
	shorttitle = {Mixed {Integer} {Programming}},
	abstract = {Back in 2001, Bixby et al. (The Sharpest Cut: The Impact of Manfred Padberg and His Work, pp. 309–325, 2004) provided an analysis of the performance impact of the main mixed integer programming features and improvements up to CPLEX 8.0 for a workshop in honor of Manfred Padberg’s 60th birthday, which was later published in a Festschrift edited by Martin Grötschel (The Sharpest Cut: The Impact of Manfred Padberg and His Work, 2004). Now, 12 years later, Grötschel’s own 65th birthday celebration seems to be the ideal opportunity to provide an update on the state of affairs.},
	language = {en},
	urldate = {2025-04-12},
	booktitle = {Facets of {Combinatorial} {Optimization}: {Festschrift} for {Martin} {Grötschel}},
	publisher = {Springer},
	author = {Achterberg, Tobias and Wunderling, Roland},
	editor = {Jünger, Michael and Reinelt, Gerhard},
	year = {2013},
	doi = {10.1007/978-3-642-38189-8_18},
	keywords = {notion},
	pages = {449--481},
}

@article{benichou_experiments_1971,
	title = {Experiments in mixed-integer linear programming},
	volume = {1},
	issn = {1436-4646},
	url = {https://doi.org/10.1007/BF01584074},
	doi = {10.1007/BF01584074},
	abstract = {This paper presents a “branch and bound” method for solving mixed integer linear programming problems. After briefly discussing the bases of the method, new concepts called pseudo-costs and estimations are introduced. Then, the heuristic rules for generating the tree, which are the main features of the method, are presented. Numerous parameters allow the user for adjusting the search strategy to a given problem.},
	language = {en},
	number = {1},
	urldate = {2025-04-12},
	journal = {Mathematical Programming},
	author = {Benichou, M. and Gauthier, J. M. and Girodet, P. and Hentges, G. and Ribiere, G. and Vincent, O.},
	month = dec,
	year = {1971},
	keywords = {Mathematical Method, Mathematical Program, Mixed Integer, notion, Programming Problem, Search Strategy},
	pages = {76--94},
}

@article{achterbergBranchingRulesRevisited2005,
  title = {Branching Rules Revisited},
  author = {Achterberg, Tobias and Koch, Thorsten and Martin, Alexander},
  year = {2005},
  month = jan,
  journal = {Operations Research Letters},
  volume = {33},
  number = {1},
  pages = {42--54},
  issn = {0167-6377},
  doi = {10.1016/j.orl.2004.04.002},
  urldate = {2025-04-12},
  abstract = {We present a new generalization called reliability branching of today's state-of-the-art strong branching and pseudocost branching strategies for linear programming based branch-and-bound algorithms. After reviewing commonly used branching strategies and performing extensive computational studies we compare different parameter settings and show the superiority of our proposed new strategy.},
  keywords = {Branch-and-bound,Mixed-integer-programming,notion,Pseudocost-branching,Reliability-branching,Strong-branching,Variable selection},
  file = {/Users/gokhanmurali/Zotero/storage/8CKRCKZH/S0167637704000501.html}
}

@inproceedings{fischettiBackdoorBranching2011,
  title = {Backdoor {{Branching}}},
  booktitle = {Integer {{Programming}} and {{Combinatoral Optimization}}},
  author = {Fischetti, Matteo and Monaci, Michele},
  editor = {G{\"u}nl{\"u}k, Oktay and Woeginger, Gerhard J.},
  year = {2011},
  pages = {183--191},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-20807-2_15},
  abstract = {Which is the minimum number of variables that need branching for a given MIP instance? Can this information be effective in producing compact branching trees, hence improving the performance of a state-of-the-art solver? In this paper we present a restart exact MIP solution scheme where a set covering model is used to find a small set of variables (a ``backdoor'', in the terminology of [8]) to be used as first-choice variables for branching. In a preliminary ``sampling'' phase, our method quickly collects a number of relevant low-cost fractional solutions that qualify as obstacles for LP bound improvement. Then a set covering model is solved to detect a small subset of variables (the backdoor) that ``cover the fractionality'' of the collected fractional solutions. These backdoor variables are put in a priority branching list, and a black-box MIP solver is eventually run---in its default mode---by taking this list into account, thus avoiding any other interference with its highly-optimized internal mechanisms. Computational results on a large set of instances from MIPLIB 2010 are presented, showing that some speedup can be achieved even with respect to a state-of-the-art solver such as IBM ILOG Cplex 12.2.},
  isbn = {978-3-642-20807-2},
  langid = {english},
  keywords = {notion}
}

@article{kilinckarzanInformationbasedBranchingSchemes2009,
  title = {Information-Based Branching Schemes for Binary Linear Mixed Integer Problems},
  author = {K{\i}l{\i}n{\c c} Karzan, Fatma and Nemhauser, George L. and Savelsbergh, Martin W. P.},
  year = {2009},
  month = dec,
  journal = {Mathematical Programming Computation},
  volume = {1},
  number = {4},
  pages = {249--293},
  issn = {1867-2949, 1867-2957},
  doi = {10.1007/s12532-009-0009-1},
  urldate = {2025-04-12},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/EIHDY8UY/Kılınç Karzan et al. - 2009 - Information-based branching schemes for binary lin.pdf}
}

@techreport{applegateFindingCutsTSP1995,
  type = {Technical {{Report}}},
  title = {Finding {{Cuts}} in the {{TSP}} ({{A}} Preliminary Report)},
  author = {Applegate, D. and Bixby, R. and Chvatal, V. and Cook, B.},
  year = {1995},
  month = mar,
  institution = {Center for Discrete Mathematics \& Theoretical Computer Science},
  abstract = {TSPLIB is Gerhard Reinelt''s library of some hundred instances of the traveling salesman problem. Some of these instances arise from drilling holes in printed circuit boards; others arise from X-ray crystallography; yet others have been constructed artificially. None of them (with a single exception) is contrived to be hard and none of them is contrived to be easy; their sizes range from 17 to 85,900 cities; some of them have been solved and others have not. We have solved twenty previously unsolved problems from the TSPLIB. One of them is the problem with 225 cities that was contrived to be hard; the sizes of the remaining nineteen range from 1,000 to 7,397 cities. Like all the successful computer programs for solving the TSP, our computer program follows the scheme designed by George Dantzig, Ray Fulkerson, and Selmer Johnson in the early nineteen-fifties. The purpose of this preliminary report is to describe *some* of our innovations in implementing the Dantzig-Fulkerson-Johnson scheme; we are planning to write up a more comprehensive account of our work soon.},
  keywords = {notion}
}

@phdthesis{DBLP:phd/de/Achterberg2007,
  author       = {Tobias Achterberg},
  title        = {Constraint integer programming},
  school       = {Berlin Institute of Technology},
  year         = {2007},
  urn          = {urn:nbn:de:kobv:83-opus-16117},
  isbn         = {978-3-89963-892-9},
  timestamp    = {Sat, 17 Jul 2021 09:07:38 +0200},
  biburl       = {https://dblp.org/rec/phd/de/Achterberg2007.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{alvarezMachineLearningBasedApproximation2017,
  title = {A {{Machine Learning-Based Approximation}} of {{Strong Branching}}},
  author = {Alvarez, Alejandro Marcos and Louveaux, Quentin and Wehenkel, Louis},
  year = {2017},
  month = jan,
  journal = {INFORMS Journal on Computing},
  volume = {29},
  number = {1},
  pages = {185--195},
  issn = {1091-9856, 1526-5528},
  doi = {10.1287/ijoc.2016.0723},
  urldate = {2023-12-22},
  abstract = {We present in this paper a new generic approach to variable branching in branch and bound for mixed-integer linear problems. Our approach consists in imitating the decisions taken by a good branching strategy, namely strong branching, with a fast approximation. This approximated function is created by a machine learning technique from a set of observed branching decisions taken by strong branching. The philosophy of the approach is similar to reliability branching. However, our approach can catch more complex aspects of observed previous branchings to take a branching decision. The experiments performed on randomly generated and MIPLIB problems show promising results.},
  langid = {english},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/M9JNLH3J/Alvarez et al_2017_A Machine Learning-Based Approximation of Strong Branching.pdf}
}

@article{khalilLearningBranchMixed2016,
  title = {Learning to {{Branch}} in {{Mixed Integer Programming}}},
  author = {Khalil, Elias and Le Bodic, Pierre and Song, Le and Nemhauser, George and Dilkina, Bistra},
  year = {2016},
  month = feb,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {30},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v30i1.10080},
  urldate = {2023-12-16},
  abstract = {The design of strategies for branching in Mixed Integer Programming (MIP) is guided by cycles of parameter tuning and offline experimentation on an extremely heterogeneous testbed, using the average performance. Once devised, these strategies (and their parameter settings) are essentially input-agnostic. To address these issues, we propose a machine learning (ML) framework for variable branching in MIP.Our method observes the decisions made by Strong Branching (SB), a time-consuming strategy that produces small search trees, collecting features that characterize the candidate branching variables at each node of the tree. Based on the collected data, we learn an easy-to-evaluate surrogate function that mimics the SB strategy, by means of solving a learning-to-rank problem, common in ML. The learned ranking function is then used for branching. The learning is instance-specific, and is performed on-the-fly while executing a branch-and-bound search to solve the MIP instance. Experiments on benchmark instances indicate that our method produces significantly smaller search trees than existing heuristics, and is competitive with a state-of-the-art commercial solver.},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/LFLAQJ4F/Khalil et al_2016_Learning to Branch in Mixed Integer Programming.pdf}
}

@misc{gasseExactCombinatorialOptimization2019,
  title = {Exact {{Combinatorial Optimization}} with {{Graph Convolutional Neural Networks}}},
  author = {Gasse, Maxime and Ch{\'e}telat, Didier and Ferroni, Nicola and Charlin, Laurent and Lodi, Andrea},
  year = {2019},
  month = oct,
  number = {arXiv:1906.01629},
  eprint = {1906.01629},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2023-12-22},
  abstract = {Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems. Code for reproducing all the experiments can be found at https://github.com/ds4dm/learn2branch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,notion,Statistics - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/6I9EEIA8/Gasse et al_2019_Exact Combinatorial Optimization with Graph Convolutional Neural Networks.pdf;/Users/gokhanmurali/Zotero/storage/KQ8WDPU8/1906.html}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-04-11},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,notion,Statistics - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/AMLGTYJS/Kipf_Welling_2017_Semi-Supervised Classification with Graph Convolutional Networks.pdf;/Users/gokhanmurali/Zotero/storage/WC8C26CA/1609.html}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-12-10},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,notion,Statistics - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/7ZF3YGI3/Veličković et al. - 2018 - Graph Attention Networks.pdf;/Users/gokhanmurali/Zotero/storage/WFWTW9WN/1710.html}
}

@article{geurtsExtremelyRandomizedTrees2006,
  title = {Extremely Randomized Trees},
  author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  year = {2006},
  month = apr,
  journal = {Machine Learning},
  volume = {63},
  number = {1},
  pages = {3--42},
  issn = {1573-0565},
  doi = {10.1007/s10994-006-6226-1},
  urldate = {2024-04-02},
  abstract = {This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
  langid = {english},
  keywords = {Bias/variance tradeoff,Cut-point randomization,Decision and regression trees,Ensemble methods,Kernel-based models,notion,Supervised learning},
  file = {/Users/gokhanmurali/Zotero/storage/FWRP7AAL/Geurts et al_2006_Extremely randomized trees.pdf}
}


@inproceedings{joachimsTrainingLinearSVMs2006,
  title = {Training Linear {{SVMs}} in Linear Time},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  author = {Joachims, Thorsten},
  year = {2006},
  month = aug,
  pages = {217--226},
  publisher = {ACM},
  address = {Philadelphia PA USA},
  doi = {10.1145/1150402.1150429},
  urldate = {2025-04-19},
  abstract = {Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for highdimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N , while each example has only s {$<<$} N non-zero features. This paper presents a Cutting-Plane Algorithm for training linear SVMs that provably has training time O(sn) for classification problems and O(sn log(n)) for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like SVMLight for large datasets.},
  isbn = {978-1-59593-339-3},
  langid = {english},
  file = {/Users/gokhanmurali/Zotero/storage/3585SXRR/Joachims - 2006 - Training linear SVMs in linear time.pdf}
}


@article{joachimsOptimizingSearchEngines,
  title = {Optimizing {{Search Engines}} Using {{Clickthrough Data}}},
  author = {Joachims, Thorsten},
  abstract = {This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.},
  langid = {english},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/AW5TUMT8/Joachims - Optimizing Search Engines using Clickthrough Data.pdf}
}


@misc{baLayerNormalization2016,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  number = {arXiv:1607.06450},
  eprint = {1607.06450},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1607.06450},
  urldate = {2025-04-19},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/AUNHSY4G/Ba et al_2016_Layer Normalization.pdf;/Users/gokhanmurali/Zotero/storage/IVSXLICN/1607.html}
}


@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2025-04-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/JM7CFARI/Kingma_Ba_2017_Adam.pdf}
}


@misc{hansknechtCutsPrimalHeuristics2018a,
  title = {Cuts, {{Primal Heuristics}}, and {{Learning}} to {{Branch}} for the {{Time-Dependent Traveling Salesman Problem}}},
  author = {Hansknecht, Christoph and Joormann, Imke and Stiller, Sebastian},
  year = {2018},
  month = may,
  number = {arXiv:1805.01415},
  eprint = {1805.01415},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1805.01415},
  urldate = {2025-04-19},
  abstract = {We consider the time-dependent traveling salesman problem (TDTSP), a generalization of the asymmetric traveling salesman problem (ATSP) to incorporate time-dependent cost functions. In our model, the costs of an arc can change arbitrarily over time (and do not only dependent on the position in the tour). The TDTSP turns out to be structurally more difficult than the TSP. We prove it is NP-hard and APX-hard even if a generalized version of the triangle inequality is satisfied. In particular, we show that even the computation of one-trees becomes intractable in the case of time-dependent costs. We derive two IP formulations of the TDTSP based on time-expansion and propose different pricing algorithms to handle the significantly in- creased problem size. We introduce multiple families of cutting planes for the TDTSP as well as different LP-based primal heuristics, a propaga- tion method and a branching rule. We conduct computational experiments to evaluate the effectiveness of our approaches on randomly generated in- stances. We are able to decrease the optimality gap remaining after one hour of computations to about six percent, compared to a gap of more than forty percent obtained by an off-the-shelf IP solver. Finally, we carry out a first attempt to learn strong branching decisions for the TDTSP. At the current state, this method does not improve the running times.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Optimization and Control,notion},
  file = {/Users/gokhanmurali/Zotero/storage/NU4MZE6J/Hansknecht et al_2018_Cuts, Primal Heuristics, and Learning to Branch for the Time-Dependent.pdf;/Users/gokhanmurali/Zotero/storage/DMREPE9S/1805.html}
}


@incollection{balasSetCoveringAlgorithms1980,
  title = {Set Covering Algorithms Using Cutting Planes, Heuristics, and Subgradient Optimization: {{A}} Computational Study},
  shorttitle = {Set Covering Algorithms Using Cutting Planes, Heuristics, and Subgradient Optimization},
  booktitle = {Combinatorial {{Optimization}}},
  author = {Balas, Egon and Ho, Andrew},
  editor = {Balinski, M. L. and Beale, E. M. L. and Dantzig, George B. and Kantorovich, L. and Koopmans, Tjalling C. and Tucker, A. W. and Wolfe, Philip and Bartels, R. and Chv{\'a}tal, V{\'a}clav and Cottle, Richard W. and Dennis, J. E. and Eaves, B. Curtis and Fletcher, R. and Korte, B. and Iri, Masao and Lemarechal, C. and Lemke, C. E. and Nemhauser, George L. and Padberg, Manfred W. and Powell, M. J. D. and Shapiro, Jeremy F. and Shapley, L. S. and Spielberg, K. and Tuy, Hoang and Walkup, D. W. and Wets, Roger and Witzgall, C. and Padberg, M. W.},
  year = {1980},
  volume = {12},
  pages = {37--60},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0120886},
  urldate = {2025-04-19},
  abstract = {We report on the implementation and computational testing of several versions of a set covering algorithm , based on the family of cutting planes from conditiona l bounds discussed in the companion paper (21 . The algorithm uses a set of heuristics to find prime covers , another set of heuristics to find feasible solutions to the dual linear program which are needed to generate cuts , and subgradient optimization to find lower bounda . It also uses implicit enumeration with some new branching rules . Each of the ingredients was implemented and tested in several versions . The variant of the algorithm that emerged as best was run on 55 randomly generated test problems (20 of them from the literature), with up to 200 constraints and 2000 variables. The results show th. algorithm to be more reliable and efficient than earlier procedures on large , sparse set covering problem ..},
  isbn = {978-3-642-00801-6 978-3-642-00802-3},
  langid = {english},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/JB7T73TU/Balas and Ho - 1980 - Set covering algorithms using cutting planes, heur.pdf}
}

@inproceedings{leyton-brownUniversalTestSuite2000,
  title = {Towards a Universal Test Suite for Combinatorial Auction Algorithms},
  booktitle = {Proceedings of the 2nd {{ACM}} Conference on {{Electronic}} Commerce},
  author = {{Leyton-Brown}, Kevin and Pearson, Mark and Shoham, Yoav},
  year = {2000},
  month = oct,
  pages = {66--76},
  publisher = {ACM},
  address = {Minneapolis Minnesota USA},
  doi = {10.1145/352871.352879},
  urldate = {2025-04-19},
  abstract = {General combinatorial auctions---auctions in which bidders place unrestricted bids for bundles of goods---are the subject of increasing study. Much of this work has focused on algorithms for finding an optimal or approximately optimal set of winning bids. Comparatively little attention has been paid to methodical evaluation and comparison of these algorithms. In particular, there has not been a systematic discussion of appropriate data sets that can serve as universally accepted and well motivated benchmarks. In this paper we present a suite of distribution families for generating realistic, economically motivated combinatorial bids in five broad real-world domains. We hope that this work will yield many comments, criticisms and extensions, bringing the community closer to a universal combinatorial auction test suite.},
  isbn = {978-1-58113-272-4},
  langid = {english},
  keywords = {notion},
  file = {/Users/gokhanmurali/Zotero/storage/JE9HTV74/Leyton-Brown et al. - 2000 - Towards a universal test suite for combinatorial a.pdf}
}


@article{cornuejolsComparisonHeuristicsRelaxations1991,
  title = {A Comparison of Heuristics and Relaxations for the Capacitated Plant Location Problem},
  author = {Cornuejols, G. and Sridharan, R. and Thizy, J. M.},
  year = {1991},
  month = feb,
  journal = {European Journal of Operational Research},
  volume = {50},
  number = {3},
  pages = {280--297},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(91)90261-S},
  urldate = {2025-04-19},
  abstract = {Approaches proposed in the literature for the Capacitated Plant Location Problem are compared. The comparison is based on new theoretical and computational results. The main emphasis is on relaxations. In particular, dominance relations among the various relaxations found in the literature are identified. In the computational study, the relaxations are compared as a function of various characteristics of the test problems. Several of these relaxations can be used to generate heuristic feasible solutions that are better than the classical greedy or interchange heuristics, both in computing time and in the quality of the solutions found.},
  keywords = {capacitated facility location,Lagrangian heuristic,Lagrangian relaxation,Location theory},
  file = {/Users/gokhanmurali/Zotero/storage/S9VQRZZF/037722179190261S.html}
}


@book{bergmanDecisionDiagramsOptimization2016,
  title = {Decision {{Diagrams}} for {{Optimization}}},
  author = {Bergman, David and Cire, Andre A. and Van Hoeve, Willem-Jan and Hooker, John},
  year = {2016},
  series = {Artificial {{Intelligence}}: {{Foundations}}, {{Theory}}, and {{Algorithms}}},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-42849-9},
  urldate = {2025-04-19},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-3-319-42847-5 978-3-319-42849-9},
  langid = {english},
  keywords = {Constraint Programming,Cutting Planes,Decision Diagrams,Discrete Optimization,Graph Coloring,Operations Research}
}


@misc{luUniversalApproximationTheorem2020,
  title = {A {{Universal Approximation Theorem}} of {{Deep Neural Networks}} for {{Expressing Probability Distributions}}},
  author = {Lu, Yulong and Lu, Jianfeng},
  year = {2020},
  month = nov,
  number = {arXiv:2004.08867},
  eprint = {2004.08867},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2004.08867},
  urldate = {2025-04-19},
  abstract = {This paper studies the universal approximation property of deep neural networks for representing probability distributions. Given a target distribution \${\textbackslash}pi\$ and a source distribution \$p\_z\$ both defined on \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$, we prove under some assumptions that there exists a deep neural network \$g:{\textbackslash}mathbb\{R\}{\textasciicircum}d{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}\$ with ReLU activation such that the push-forward measure \$({\textbackslash}nabla g)\_{\textbackslash}\# p\_z\$ of \$p\_z\$ under the map \${\textbackslash}nabla g\$ is arbitrarily close to the target measure \${\textbackslash}pi\$. The closeness are measured by three classes of integral probability metrics between probability distributions: \$1\$-Wasserstein distance, maximum mean distance (MMD) and kernelized Stein discrepancy (KSD). We prove upper bounds for the size (width and depth) of the deep neural network in terms of the dimension \$d\$ and the approximation error \${\textbackslash}varepsilon\$ with respect to the three discrepancies. In particular, the size of neural network can grow exponentially in \$d\$ when \$1\$-Wasserstein distance is used as the discrepancy, whereas for both MMD and KSD the size of neural network only depends on \$d\$ at most polynomially. Our proof relies on convergence estimates of empirical measures under aforementioned discrepancies and semi-discrete optimal transport.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis,Mathematics - Statistics Theory,Statistics - Machine Learning,Statistics - Statistics Theory}
}


@misc{prouvostEcoleGymlikeLibrary2020a,
  title = {Ecole: {{A Gym-like Library}} for {{Machine Learning}} in {{Combinatorial Optimization Solvers}}},
  shorttitle = {Ecole},
  author = {Prouvost, Antoine and Dumouchelle, Justin and Scavuzzo, Lara and Gasse, Maxime and Ch{\'e}telat, Didier and Lodi, Andrea},
  year = {2020},
  month = nov,
  number = {arXiv:2011.06069},
  eprint = {2011.06069},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2011.06069},
  urldate = {2025-04-19},
  abstract = {We present Ecole, a new library to simplify machine learning research for combinatorial optimization. Ecole exposes several key decision tasks arising in general-purpose combinatorial optimization solvers as control problems over Markov decision processes. Its interface mimics the popular OpenAI Gym library and is both extensible and intuitive to use. We aim at making this library a standardized platform that will lower the bar of entry and accelerate innovation in the field. Documentation and code can be found at https://www.ecole.ai.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,notion},
  file = {/Users/gokhanmurali/Zotero/storage/FITVKJ67/Prouvost et al_2020_Ecole.pdf}
}

@misc{feyFastGraphRepresentation2019,
  title = {Fast {{Graph Representation Learning}} with {{PyTorch Geometric}}},
  author = {Fey, Matthias and Lenssen, Jan Eric},
  year = {2019},
  month = apr,
  number = {arXiv:1903.02428},
  eprint = {1903.02428},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1903.02428},
  urldate = {2025-04-19},
  abstract = {We introduce PyTorch Geometric, a library for deep learning on irregularly structured input data such as graphs, point clouds and manifolds, built upon PyTorch. In addition to general graph data structures and processing methods, it contains a variety of recently published methods from the domains of relational learning and 3D data processing. PyTorch Geometric achieves high data throughput by leveraging sparse GPU acceleration, by providing dedicated CUDA kernels and by introducing efficient mini-batch handling for input examples of different size. In this work, we present the library in detail and perform a comprehensive comparative study of the implemented methods in homogeneous evaluation scenarios.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/gokhanmurali/Zotero/storage/F3NMSTSK/Fey_Lenssen_2019_Fast Graph Representation Learning with PyTorch Geometric.pdf}
}
